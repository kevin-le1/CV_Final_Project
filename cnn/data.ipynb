{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carter/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/carter/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for Kili/plastic_in_river contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Kili/plastic_in_river\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'litter'],\n",
      "        num_rows: 3407\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'litter'],\n",
      "        num_rows: 427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'litter'],\n",
      "        num_rows: 425\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Kili/plastic_in_river', num_proc=6)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting data in split test: 100%|██████████| 427/427 [01:57<00:00,  3.64it/s]\n",
      "Sorting data in split validation: 100%|██████████| 425/425 [01:50<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# only creating datasets for train and validation not test\n",
    "os.makedirs('datasets/images/train', exist_ok=True)\n",
    "os.makedirs('datasets/images/test', exist_ok=True)\n",
    "os.makedirs('datasets/images/validation', exist_ok=True)\n",
    "\n",
    "CLEAN = 0\n",
    "OIL_SPILL = 1 # not used here\n",
    "TRASH = 2\n",
    "\n",
    "class_labels = [CLEAN, OIL_SPILL, TRASH]\n",
    "\n",
    "def create_dataset(data, split):\n",
    "    data = data[split]\n",
    "    data_size = len(data)\n",
    "\n",
    "    # create class sub dirs\n",
    "    for label in class_labels:\n",
    "        os.makedirs(f'datasets/images/{split}/{label}', exist_ok=True)\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(data, total=data_size, desc=f\"Sorting data in split {split}\")):\n",
    "        image = sample['image']\n",
    "        labels = sample['litter']['label']\n",
    "        # targets = []\n",
    "        \n",
    "        if(len(labels) > 0):\n",
    "            class_id = TRASH\n",
    "        else:\n",
    "            class_id = CLEAN\n",
    "\n",
    "        # saving image as png to directory\n",
    "        image.save(f'datasets/images/{split}/{class_id}/{idx}.png')\n",
    "\n",
    "\n",
    "splits = [\"train\", \"test\", \"validation\"]\n",
    "\n",
    "for split in splits:\n",
    "    create_dataset(dataset, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted 256 images\t 256 == 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "START_DIR = \"oil/\"\n",
    "TARGET_WORD = \"oil\"\n",
    "\n",
    "matching_directories = [os.path.join(dirpath, dirname) for dirpath, dirnames, _ in os.walk(START_DIR) for dirname in dirnames if TARGET_WORD in dirname]\n",
    "\n",
    "img_ordering = []\n",
    "\n",
    "for dir in matching_directories:\n",
    "  # go through all images in subdirs\n",
    "  imgs = [file for file in glob.iglob(dir + '/*.jpg')]\n",
    "  for img in imgs:\n",
    "    img_ordering.append(img) # create total ordering of imgs\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(img_ordering)\n",
    "\n",
    "img_count = len(img_ordering)\n",
    "\n",
    "train_data = img_ordering[:int((img_count+1)*.60)]\n",
    "val_data = img_ordering[int((img_count+1)*.60):int((img_count+1)*.80)]\n",
    "test_data = img_ordering[int((img_count+1)*.80):]\n",
    "\n",
    "datas = [(train_data, \"train\"), (val_data, \"validation\"), (test_data, \"test\")]\n",
    "sorted_img_count = 0\n",
    "for data, label in datas:\n",
    "  os.makedirs(f'datasets/images/{label}/1', exist_ok=True)\n",
    "  idx = 0\n",
    "  for img in data:\n",
    "    shutil.copy(img, f\"datasets/images/{label}/1/{idx}.jpg\")\n",
    "    sorted_img_count += 1\n",
    "    idx += 1\n",
    "\n",
    "print(f\"Sorted {img_count} images\\t {img_count} == {sorted_img_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5307e98954c1652c3564abd66d0211a7c00ac63b2731b8c6069a552ec199cec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
